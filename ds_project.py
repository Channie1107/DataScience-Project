# -*- coding: utf-8 -*-
"""DS project

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1l-idf5ZRCh_d7oh9KDXp9DKDoGlVnZys
"""

from google.colab import drive
drive.mount ('/content/drive')

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from imblearn.over_sampling import SMOTE
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix
from sklearn.model_selection import KFold

"""DATA DESCRIPTION + DATA CLEANING"""

df = pd.read_csv("/content/drive/My Drive/email.csv")
df

drop_index = 5572
df = df.drop(drop_index)
print(df)

df.info()

df.isna().sum()

df.duplicated().sum()

df.drop_duplicates(inplace=True)
df.duplicated().sum()

df.shape

count_category = df['Category'].value_counts()
count_category

"""#DATA DISTRIBUTION"""

sns.countplot(df, x='Category')
plt.xlabel('Category')
plt.ylabel('count')
plt.title('Distribution of mails')
plt.show()

unique_categories = df['Category'].unique()
print(unique_categories)

plt.pie(df['Category'].value_counts(),labels=['ham','spam'],autopct='%0.2f')
plt.show()

"""TEXT PROCESSING"""

df['spam'] = df['Category'].apply(lambda x:1 if x=='spam' else 0)

df['Message'][10]

import string

def preprocessing(text):
    text = text.lower()

    for txt in text:
        if txt in string.punctuation:
            text = text.replace(txt,"").strip()

    return text

df['Message'] = df['Message'].apply(preprocessing)
print(df['Message'][10])

import nltk
from nltk.corpus import stopwords
import string

nltk.download('stopwords')

def remove_stopwords(text):
    stop_words = set(stopwords.words('english'))  # Use a set for faster lookup
    text = text.lower()  # Convert text to lowercase
    words = text.split()  # Split text into words
    words = [word for word in words if word not in stop_words]  # Remove stopwords
    return ' '.join(words)

# Example usage
text = "This is a sample sentence, showing off the stop words filtration."
clean_text = remove_stopwords(text)
print(clean_text)

remove_stopwords("im gonna be home soon and i dont want to talk about this stuff anymore tonight k ive cried enough today")

df['Message'] = df['Message'].apply(remove_stopwords)

from nltk.stem.porter import PorterStemmer
ps = PorterStemmer()

def steaming(text):
    text = nltk.word_tokenize(text)
    lst = []
    for i in text:
        lst.append(ps.stem(i))
    return ' '.join(lst)

nltk.download('stopwords')
nltk.download('punkt')

steaming("im gonna home soon dont want talk stuff anymore tonight k ive cried enough today")

df['Message'] = df['Message'].apply(steaming)

df.drop(columns=['Category'],inplace=True)
df.rename(columns={'spam':'Category'},inplace=True)

from wordcloud import WordCloud

wc = WordCloud(width=500,height=500,min_font_size=10,background_color='white')

spam_wc = wc.generate(df[df['Category']==1]['Message'].str.cat(sep=" "))
plt.imshow(spam_wc)
plt.show()

ham_wc = wc.generate(df[df['Category']==0]['Message'].str.cat(sep=" "))
plt.imshow(ham_wc)
plt.show()

spam_corpus = []
for msg in df[df['Category'] == 1]['Message'].tolist():
    for word in msg.split():
        spam_corpus.append(word)
len(spam_corpus)

from collections import Counter
plt.figure(figsize=(20,8))
sns.barplot(x=pd.DataFrame(Counter(spam_corpus).most_common(30))[0],y=pd.DataFrame(Counter(spam_corpus).most_common(30))[1],palette='dark')
plt.xticks(rotation=90)
plt.show()

ham_corpus = []
for msg in df[df['Category'] == 0]['Message'].tolist():
    for word in msg.split():
        ham_corpus.append(word)
len(ham_corpus)

plt.figure(figsize=(20,8))
sns.barplot(x=pd.DataFrame(Counter(ham_corpus).most_common(30))[0],y=pd.DataFrame(Counter(ham_corpus).most_common(30))[1],palette='dark')
plt.xticks(rotation=90)
plt.show()

"""MODELING

LABEL
"""

df.loc[df['Category'] == 'ham', 'Category'] = 0
df.loc[df['Category'] == 'spam', 'Category'] = 1

df.head()

X = df["Message"]
Y = df["Category"]

print(X)

print(Y)

"""*SPLIT* DATA"""

feature_extraction = TfidfVectorizer(min_df=1, stop_words="english", lowercase=True)

X_features = feature_extraction.fit_transform(X)

Y = Y.astype('int')

print(X_features)

print(X)

kf = KFold(n_splits=5, shuffle=True, random_state=42)
accuracies = []
precisions = []
recalls = []
f1_scores = []

model = LogisticRegression()

for train_index, test_index in kf.split(X_features, Y):
    X_train, X_test = X_features[train_index], X_features[test_index]
    y_train, y_test = Y.iloc[train_index], Y.iloc[test_index]

    # Thực hiện oversampling trên tập huấn luyện bằng SMOTE
    smote = SMOTE(random_state=42)
    X_train_oversampled, y_train_oversampled = smote.fit_resample(X_train, y_train)

    # Huấn luyện mô hình trên dữ liệu mới có oversampling
    model.fit(X_train_oversampled, y_train_oversampled)

    # Đánh giá mô hình trên fold hiện tại (tập kiểm tra)
    y_pred = model.predict(X_test)

    accuracy = accuracy_score(y_test, y_pred)
    accuracies.append(accuracy)

    report = classification_report(y_test, y_pred, output_dict=True)

    precisions.append(report['weighted avg']['precision'])
    recalls.append(report['weighted avg']['recall'])
    f1_scores.append(report['weighted avg']['f1-score'])

    # In báo cáo phân loại và ma trận nhầm lẫn cho từng fold
    print(report)
    print('\n')
    cm = confusion_matrix(y_test, y_pred)
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
    plt.xlabel('Predicted')
    plt.ylabel('Actual')
    plt.title(f'Confusion Matrix for Fold {len(accuracies)}')
    plt.show()

# Tính trung bình các giá trị accuracy, precision, recall và F1-score
mean_accuracy = np.mean(accuracies)
mean_precision = np.mean(precisions)
mean_recall = np.mean(recalls)
mean_f1_score = np.mean(f1_scores)

print("Mean Accuracy:", mean_accuracy)
print("Mean Precision:", mean_precision)
print("Mean Recall:", mean_recall)
print("Mean F1-Score:", mean_f1_score)

